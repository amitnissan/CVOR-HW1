{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51bf7f04-b564-482a-a5dd-87434495dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report, average_precision_score, f1_score, accuracy_score\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import bbox_visualizer as bbv\n",
    "from yolov7_package import Yolov7Detector\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1835c5f-ace2-446b-bbba-cd909ca81b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "labels = open('YOLOv7/classes.names').read().strip().split('\\n')\n",
    "model = Yolov7Detector(weights='yolov7-old/runs/train/yolov7-cvor9/weights/best.pt', img_size=[640, 480], classes='YOLOv7/classes.names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240bb817-a08c-45a4-be7c-3e1dcf4bea33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_image(image_filepath):\n",
    "    try:\n",
    "        classes, boxes, scores = model.detect(image_filepath)\n",
    "        frame = image_filepath\n",
    "    except:\n",
    "        frame = cv.imread(image_filepath)\n",
    "        classes, boxes, scores = model.detect(frame)\n",
    "    img = model.draw_on_image(frame, boxes[0], scores[0], classes[0])\n",
    "    cv.imwrite(f'models/results/{image_filepath}.jpg', frame)\n",
    "    return classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c266185-d2a0-449c-a6ff-5ca1bb3b9188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f273db8c7c463fb6453752f9b3eab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred, y_true = [], []\n",
    "for file in tqdm_notebook(os.listdir('YOLOv7/test/images')):\n",
    "    cur_pred = predict_image(f'YOLOv7/test/images/{file}')\n",
    "    if (len(cur_pred)==2):\n",
    "        y_pred.append(cur_pred)\n",
    "    elif(len(cur_pred)>2):\n",
    "        y_pred.append([cur_pred[0], cur_pred[1]])\n",
    "    else:\n",
    "        new_pred = y_pred.pop(len(y_pred)-1)\n",
    "        y_pred.insert(len(y_pred)-1, new_pred)\n",
    "        y_pred.insert(len(y_pred)-1, new_pred)\n",
    "    labels_filename = file.split('.')[0]\n",
    "    current_y_true = []\n",
    "    with open (f'YOLOv7/test/labels/{labels_filename}.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            current_y_true.append(int(line[0]))\n",
    "    y_true.append(current_y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b303290a-de2e-47fd-a564-394988f04042",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [sorted(element) for element in y_true]\n",
    "y_pred = [sorted(element) for element in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3eb65be-020a-421e-a715-3be81b6e2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [item for sublist in y_true for item in sublist]\n",
    "y_pred = [item for sublist in y_pred for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd98b664-8505-48f5-97bf-e2486bace884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83        42\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.95      0.92      0.93       137\n",
      "           3       0.60      0.30      0.40        10\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.75      0.97      0.85        62\n",
      "           6       0.87      0.65      0.74        20\n",
      "           7       0.93      0.89      0.91       126\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       398\n",
      "   macro avg       0.61      0.57      0.58       398\n",
      "weighted avg       0.88      0.88      0.88       398\n",
      "\n",
      "f1 macro: 0.6656308873304708\n",
      "Accuracy: 0.8793969849246231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, labels=[0,1,2,3,4,5,6,7]))\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "print(f'f1 macro: {f1_macro}')\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf2645-6245-4bc2-bda4-9ab60b52e9a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [mAP for Boxes](https://github.com/ZFTurbo/Mean-Average-Precision-for-Boxes)\n",
    "#### y_true = ['ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax']\n",
    "#### y_pred = ['ImageID', 'LabelName', 'Conf', 'XMin', 'XMax', 'YMin', 'YMax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391c48ba-731b-4e69-bee2-4b5b668964bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Roman Solovyev, IPPM RAS\n",
    "URL: https://github.com/ZFTurbo\n",
    "\n",
    "Code based on: https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/utils/eval.py\n",
    "\"\"\"\n",
    "\n",
    "def compute_overlap(boxes, query_boxes):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        a: (N, 4) ndarray of float\n",
    "        b: (K, 4) ndarray of float\n",
    "\n",
    "    Returns\n",
    "        overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    N = boxes.shape[0]\n",
    "    K = query_boxes.shape[0]\n",
    "    overlaps = np.zeros((N, K), dtype=np.float64)\n",
    "    for k in range(K):\n",
    "        box_area = (\n",
    "            (query_boxes[k, 2] - query_boxes[k, 0]) *\n",
    "            (query_boxes[k, 3] - query_boxes[k, 1])\n",
    "        )\n",
    "        for n in range(N):\n",
    "            iw = (\n",
    "                min(boxes[n, 2], query_boxes[k, 2]) -\n",
    "                max(boxes[n, 0], query_boxes[k, 0])\n",
    "            )\n",
    "            if iw > 0:\n",
    "                ih = (\n",
    "                    min(boxes[n, 3], query_boxes[k, 3]) -\n",
    "                    max(boxes[n, 1], query_boxes[k, 1])\n",
    "                )\n",
    "                if ih > 0:\n",
    "                    ua = np.float64(\n",
    "                        (boxes[n, 2] - boxes[n, 0]) *\n",
    "                        (boxes[n, 3] - boxes[n, 1]) +\n",
    "                        box_area - iw * ih\n",
    "                    )\n",
    "                    overlaps[n, k] = iw * ih / ua\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def get_real_annotations(table):\n",
    "    res = dict()\n",
    "    ids = table['ImageID'].values.astype('str')\n",
    "    labels = table['LabelName'].values.astype('str')\n",
    "    xmin = table['XMin'].values.astype(np.float32)\n",
    "    xmax = table['XMax'].values.astype(np.float32)\n",
    "    ymin = table['YMin'].values.astype(np.float32)\n",
    "    ymax = table['YMax'].values.astype(np.float32)\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        id = ids[i]\n",
    "        label = labels[i]\n",
    "        if id not in res:\n",
    "            res[id] = dict()\n",
    "        if label not in res[id]:\n",
    "            res[id][label] = []\n",
    "        box = [xmin[i], ymin[i], xmax[i], ymax[i]]\n",
    "        res[id][label].append(box)\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_detections(table):\n",
    "    res = dict()\n",
    "    ids = table['ImageID'].values.astype('str')\n",
    "    labels = table['LabelName'].values.astype('str')\n",
    "    scores = table['Conf'].values.astype(np.float32)\n",
    "    xmin = table['XMin'].values.astype(np.float32)\n",
    "    xmax = table['XMax'].values.astype(np.float32)\n",
    "    ymin = table['YMin'].values.astype(np.float32)\n",
    "    ymax = table['YMax'].values.astype(np.float32)\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        id = ids[i]\n",
    "        label = labels[i]\n",
    "        if id not in res:\n",
    "            res[id] = dict()\n",
    "        if label not in res[id]:\n",
    "            res[id][label] = []\n",
    "        box = [xmin[i], ymin[i], xmax[i], ymax[i], scores[i]]\n",
    "        res[id][label].append(box)\n",
    "\n",
    "    return res\n",
    "\n",
    "def _compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "def mean_average_precision_for_boxes(ann, pred, iou_threshold=0.5, exclude_not_in_annotations=False, verbose=True):\n",
    "    \"\"\"\n",
    "\n",
    "    :param ann: path to CSV-file with annotations or numpy array of shape (N, 6)\n",
    "    :param pred: path to CSV-file with predictions (detections) or numpy array of shape (N, 7)\n",
    "    :param iou_threshold: IoU between boxes which count as 'match'. Default: 0.5\n",
    "    :param exclude_not_in_annotations: exclude image IDs which are not exist in annotations. Default: False\n",
    "    :param verbose: print detailed run info. Default: True\n",
    "    :return: tuple, where first value is mAP and second values is dict with AP for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(ann, str):\n",
    "        valid = pd.read_csv(ann)\n",
    "    else:\n",
    "        valid = pd.DataFrame(ann, columns=['ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax'])\n",
    "\n",
    "    if isinstance(pred, str):\n",
    "        preds = pd.read_csv(pred)\n",
    "    else:\n",
    "        preds = pd.DataFrame(pred, columns=['ImageID', 'LabelName', 'Conf', 'XMin', 'XMax', 'YMin', 'YMax'])\n",
    "\n",
    "    ann_unique = valid['ImageID'].unique()\n",
    "    preds_unique = preds['ImageID'].unique()\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of files in annotations: {}'.format(len(ann_unique)))\n",
    "        print('Number of files in predictions: {}'.format(len(preds_unique)))\n",
    "\n",
    "    # Exclude files not in annotations!\n",
    "    if exclude_not_in_annotations:\n",
    "        preds = preds[preds['ImageID'].isin(ann_unique)]\n",
    "        preds_unique = preds['ImageID'].unique()\n",
    "        if verbose:\n",
    "            print('Number of files in detection after reduction: {}'.format(len(preds_unique)))\n",
    "\n",
    "    unique_classes = valid['LabelName'].unique().astype('str')# gabrielg changed astype(np.str)\n",
    "    if verbose:\n",
    "        print('Unique classes: {}'.format(len(unique_classes)))\n",
    "\n",
    "    all_detections = get_detections(preds)\n",
    "    all_annotations = get_real_annotations(valid)\n",
    "    if verbose:\n",
    "        print('Detections length: {}'.format(len(all_detections)))\n",
    "        print('Annotations length: {}'.format(len(all_annotations)))\n",
    "    print('-----------------')\n",
    "    print(f'AP@{iou_threshold} per Class')\n",
    "    print('-----------------')\n",
    "    average_precisions = {}\n",
    "    for zz, label in enumerate(sorted(unique_classes)):\n",
    "\n",
    "        # Negative class\n",
    "        if str(label) == 'nan':\n",
    "            continue\n",
    "\n",
    "        false_positives = []\n",
    "        true_positives = []\n",
    "        scores = []\n",
    "        num_annotations = 0.0\n",
    "\n",
    "        for i in range(len(ann_unique)):\n",
    "            detections = []\n",
    "            annotations = []\n",
    "            id = ann_unique[i]\n",
    "            if id in all_detections:\n",
    "                if label in all_detections[id]:\n",
    "                    detections = all_detections[id][label]\n",
    "            if id in all_annotations:\n",
    "                if label in all_annotations[id]:\n",
    "                    annotations = all_annotations[id][label]\n",
    "\n",
    "            if len(detections) == 0 and len(annotations) == 0:\n",
    "                continue\n",
    "\n",
    "            num_annotations += len(annotations)\n",
    "            detected_annotations = []\n",
    "\n",
    "            annotations = np.array(annotations, dtype=np.float64)\n",
    "            for d in detections:\n",
    "                scores.append(d[4])\n",
    "\n",
    "                if len(annotations) == 0:\n",
    "                    false_positives.append(1)\n",
    "                    true_positives.append(0)\n",
    "                    continue\n",
    "\n",
    "                overlaps = compute_overlap(np.expand_dims(np.array(d, dtype=np.float64), axis=0), annotations)\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                max_overlap = overlaps[0, assigned_annotation]\n",
    "\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                    false_positives.append(0)\n",
    "                    true_positives.append(1)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else:\n",
    "                    false_positives.append(1)\n",
    "                    true_positives.append(0)\n",
    "\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0, 0\n",
    "            continue\n",
    "        \n",
    "        false_positives = np.array(false_positives)\n",
    "        true_positives = np.array(true_positives)\n",
    "        scores = np.array(scores)\n",
    "\n",
    "        # sort by score\n",
    "        indices = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives = true_positives[indices]\n",
    "\n",
    "        # compute false positives and true positives\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives = np.cumsum(true_positives)\n",
    "\n",
    "        # compute recall and precision\n",
    "        recall = true_positives / num_annotations\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision = _compute_ap(recall, precision)\n",
    "        average_precisions[label] = average_precision, num_annotations\n",
    "        if verbose:\n",
    "            s1 = \"{:30s} | {:.6f} | {:7d}\".format(label, average_precision, int(num_annotations))\n",
    "            print(s1)\n",
    "\n",
    "    present_classes = 0\n",
    "    precision = 0\n",
    "    for label, (average_precision, num_annotations) in average_precisions.items():\n",
    "        if num_annotations > 0:\n",
    "            present_classes += 1\n",
    "            precision += average_precision\n",
    "    mean_ap = precision / present_classes\n",
    "    if verbose:\n",
    "        print('-----------------')\n",
    "        print(f'mAP@{iou_threshold}: {mean_ap}')#gabrielg changed{:.6f}'.format(mean_ap))\n",
    "        print('-----------------\\n')\n",
    "    return mean_ap, average_precisions\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935befd3-31ab-4d92-81bc-280fb2e19433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_image_for_map_calculation(image_filepath):\n",
    "    try:\n",
    "        classes, boxes, scores = model.detect(image_filepath)\n",
    "        frame = image_filepath\n",
    "    except:\n",
    "        frame = cv.imread(image_filepath)\n",
    "        classes, boxes, scores = model.detect(frame)\n",
    "    img = model.draw_on_image(frame, boxes[0], scores[0], classes[0])\n",
    "    cv.imwrite(f'models/results/{image_filepath}.jpg', frame)\n",
    "    return classes[0], boxes[0], scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b37fdfd-d80d-4055-95e5-2d77eb5d6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred_and_y_true(iou_th):\n",
    "    model = Yolov7Detector(weights='yolov7-old/runs/train/yolov7-cvor9/weights/best.pt', img_size=[416, 416], iou_thres=iou_th, classes='YOLOv7/classes.names')\n",
    "    y_pred, y_true = [], []\n",
    "    labels = open('YOLOv7/classes.names').read().strip().split('\\n')\n",
    "    print ('---------------------------------')\n",
    "    print(f'Predicting for IoU Threshold={iou_th}')\n",
    "    print ('---------------------------------')\n",
    "    for file in tqdm_notebook(os.listdir('YOLOv7/test/images')):\n",
    "        # normalize as in detect from yolov7_package\n",
    "        frame = cv.imread(f'YOLOv7/test/images/{file}')\n",
    "        \n",
    "        img_shape = frame.shape\n",
    "        w_img = img_shape[1]\n",
    "        h_img = img_shape[0]\n",
    "        \n",
    "        classes, boxes, scores = predict_image_for_map_calculation(f'YOLOv7/test/images/{file}') \n",
    "        for i in range(len(classes)):\n",
    "            xmin = boxes[i][0]#/dy if (boxes[i][0]<old_shape[1]) else 1.0\n",
    "            ymin = boxes[i][1]#/dx if (boxes[i][1]<old_shape[0]) else 1.0\n",
    "            xmax = boxes[i][2]#/dy if (boxes[i][2]<old_shape[1]) else 1.0\n",
    "            ymax = boxes[i][3]#/dx if (boxes[i][3]<old_shape[0]) else 1.0\n",
    "            class_id = classes[i]\n",
    "            conf = scores[i]\n",
    "            \n",
    "            # ['ImageID', 'LabelName', 'Conf', 'XMin', 'XMax', 'YMin', 'YMax']\n",
    "            cur_pred = [file, labels[class_id], conf, xmin, xmax, ymin, ymax]\n",
    "            y_pred.append(cur_pred)\n",
    "\n",
    "        # y_true\n",
    "        labels_filename = file.split('.')[0]\n",
    "        with open (f'YOLOv7/test/labels/{labels_filename}.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "                \n",
    "                class_id = int(line[0])\n",
    "                \n",
    "                x_cnt  = float(line[1])*w_img\n",
    "                y_cnt  = float(line[2])*h_img\n",
    "                w_bbox = float(line[3])*w_img\n",
    "                h_bbox = float(line[4])*h_img                \n",
    "                \n",
    "                xmin = x_cnt - w_bbox/2\n",
    "                ymin = y_cnt - h_bbox/2\n",
    "                xmax = xmin  + w_bbox\n",
    "                ymax = ymin  + h_bbox\n",
    "                \n",
    "                # ['ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax']\n",
    "                cur_true = [file, labels[class_id], xmin, xmax, ymin, ymax]\n",
    "                y_true.append(cur_true)\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    iou_th_str = str(iou_th).split('.')[1]\n",
    "    with open(f'mAP_for_boxes/results/y_pred_{iou_th_str}.csv', 'w', newline='') as f:\n",
    "        f_write = csv.writer(f)\n",
    "        f_write.writerow([\"ImageID\",\"LabelName\",\"Conf\",\"XMin\",\"XMax\",\"YMin\",\"YMax\"])\n",
    "        for i in range(len(y_pred)):\n",
    "            # ['ImageID', 'LabelName', 'Conf', 'XMin', 'XMax', 'YMin', 'YMax']\n",
    "            f_write.writerow([y_pred[i][0], y_pred[i][1], y_pred[i][2], y_pred[i][3], y_pred[i][4], y_pred[i][5], y_pred[i][6]])\n",
    "        \n",
    "    with open(f'mAP_for_boxes/results/y_true_{iou_th_str}.csv', 'w', newline='') as f:\n",
    "        f_write = csv.writer(f)\n",
    "        f_write.writerow([\"ImageID\",\"LabelName\",\"XMin\",\"XMax\",\"YMin\",\"YMax\"])\n",
    "        for i in range(len(y_true)):\n",
    "            # ['ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax']\n",
    "            f_write.writerow([y_true[i][0], y_true[i][1], y_true[i][2], y_true[i][3], y_true[i][4], y_true[i][5]])\n",
    "        \n",
    "    y_true = pd.read_csv(f'mAP_for_boxes/results/y_true_{iou_th_str}.csv')\n",
    "    y_pred = pd.read_csv(f'mAP_for_boxes/results/y_pred_{iou_th_str}.csv')\n",
    "    y_true = y_true[['ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax']].values\n",
    "    y_pred = y_pred[['ImageID', 'LabelName', 'Conf', 'XMin', 'XMax', 'YMin', 'YMax']].values\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d6d1ab-7cf6-44b0-861a-711dfd6bf398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "---------------------------------\n",
      "Predicting for IoU Threshold=0.25\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8ecbb870b0497f8706c97139facf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in annotations: 199\n",
      "Number of files in predictions: 199\n",
      "Unique classes: 7\n",
      "Detections length: 199\n",
      "Annotations length: 199\n",
      "-----------------\n",
      "AP@0.25 per Class\n",
      "-----------------\n",
      "Left_Empty                     | 0.000000 |     126\n",
      "Left_Forceps                   | 0.000000 |      62\n",
      "Left_Needle_driver             | 0.000000 |      10\n",
      "Left_Scissors                  | 0.000000 |       1\n",
      "Right_Empty                    | 0.005556 |      20\n",
      "Right_Needle_driver            | 0.001335 |     137\n",
      "Right_Scissors                 | 0.000000 |      42\n",
      "-----------------\n",
      "mAP@0.25: 0.000984325603720807\n",
      "-----------------\n",
      "\n",
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "---------------------------------\n",
      "Predicting for IoU Threshold=0.5\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c1385ec89341cebdff8009c8157042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in annotations: 199\n",
      "Number of files in predictions: 199\n",
      "Unique classes: 7\n",
      "Detections length: 199\n",
      "Annotations length: 199\n",
      "-----------------\n",
      "AP@0.5 per Class\n",
      "-----------------\n",
      "Left_Empty                     | 0.000000 |     126\n",
      "Left_Forceps                   | 0.000000 |      62\n",
      "Left_Needle_driver             | 0.000000 |      10\n",
      "Left_Scissors                  | 0.000000 |       1\n",
      "Right_Empty                    | 0.000000 |      20\n",
      "Right_Needle_driver            | 0.000000 |     137\n",
      "Right_Scissors                 | 0.000000 |      42\n",
      "-----------------\n",
      "mAP@0.5: 0.0\n",
      "-----------------\n",
      "\n",
      "Fusing layers... \n",
      "IDetect.fuse\n",
      "---------------------------------\n",
      "Predicting for IoU Threshold=0.75\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75b9347a83e4e9ba7091026602572c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in annotations: 199\n",
      "Number of files in predictions: 199\n",
      "Unique classes: 7\n",
      "Detections length: 199\n",
      "Annotations length: 199\n",
      "-----------------\n",
      "AP@0.75 per Class\n",
      "-----------------\n",
      "Left_Empty                     | 0.000000 |     126\n",
      "Left_Forceps                   | 0.000000 |      62\n",
      "Left_Needle_driver             | 0.000000 |      10\n",
      "Left_Scissors                  | 0.000000 |       1\n",
      "Right_Empty                    | 0.000000 |      20\n",
      "Right_Needle_driver            | 0.000000 |     137\n",
      "Right_Scissors                 | 0.000000 |      42\n",
      "-----------------\n",
      "mAP@0.75: 0.0\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iou_th in [0.25, 0.5, 0.75]:\n",
    "    y_true, y_pred = get_y_pred_and_y_true(iou_th)\n",
    "    mean_ap, average_precisions = mean_average_precision_for_boxes(y_true, y_pred, iou_threshold=iou_th)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
